---
title: RNAseq data for Ex009, Pi starvation time course in _C. glabrata_
author: Bin He
date: 10 oct 2017
---

# Sample

This folder contains raw sequencing data and processed data from the experiment Ex009
40 samples pooled into 2 pools, using 20 adapter indices for each 

# Notes

## [30 oct 2017] Gene induction analysis 2

### Goal

- Compare gene lists identified this time vs previously in the pho80Δ background 

### Voom transformation

- voom takes in either normalized counts (may be given as a DGEList object) or raw counts (with an option to normalize within voom) and a design matrix.
- It first transforms the counts by adding a pseudocount (called prior-count in voom context, default 0.5), then dividing by the (effective) library size and times 1e+6. Essentially it does the same job as the `cpm()` function with `prior.count = 0.5`.
- It then uses `lmFit` to regress the log cpm value against the design matrix, which gives the coefficients (main effect) of each factor and a residual variance for each gene in every sample.
- The plot `voom(plot=TRUE)` generates takes as its x-value (called `sx` in the function) the mean expression level of a gene as calculated by the fitted $\mu$ from the linear model and reverse-transform it into # of reads, and for its y-value (`sy`), it uses the square root of the fitted standard deviation for each gene. The resulting graph typically shows an inverse correlation between the two variables plotted, suggesting that the log2 transformation "stabilized" variance for highly expressed genes but also inflated variance for lowly expressed ones.
- The plot generated by `voom()` uses Lowess regression. The function it estimates is then used to produce a precision weight for each gene x sample. This is calculated in the following way:

	```r
	fitted.cpm <- 2^fitted.values # fitted.values is calculated for Y_{g,k} = expression of gene g in sample k from the linear model
	fitted.count <- 1e-6 * t(t(fitted.cpm)*(lib.sizes+1))
	fitted.logcount <- log2(fitted.count) # essentially it takes the fitted log count per million data and turn it into log of fitted read counts (not adjusted by library size)

	# here is where the lowess regression comes in
	l <- lowess(sx, sy, f = span)
	f <- approxfun(l, rule = 2) # this stores the lowess fit as an approximated function
	w <- 1/f(fitted.logcount)^4 # my understanding is that this takes in the fitted log counts (not lcpm though) and predict the square root of standard deviation based on the Lowess fit, then take it to the fourth power to recover the variance. the precision weight is the inverse of the predicted variance
	```

	Here is what voom actually outputs:

	```r
	out$E <- y # this is the log cpm value
	out$weights <- w
	out$design <- design # this is input by the user
	out$targets <- data.frame(lib.size = lib.size) # the "targets" object stores the library size
	new("EList", out) # finally this constructs a new "EList" object and return to the user
	```

## [27 oct 2017] Gene induction analysis 1

### Goal

- Write a function to visualize single gene time course data
- Perform differential expression (DE) analysis to identify gene sets of interests

### Notes

- used MDS plot to project the high dimensional data (~5000 genes x 40 samples) onto a small number of dimensions, similar to PCA, but using metrics better suited for microarray (or transformed RNAseq) data.
- from MDS plot, it seems like Pho4-dependent gene expression account for just a small fraction of the transcriptional response (see Rnotebook for details)
- also wrote a function `myGenePlot` in a separate R script file under `code` directory.
- examined four groups of Pho4 targets identified in my 2017 eLife paper. The first two, corresponding to the conserved and Cg specific phosphate homeostasis related genes, are clearly induced in a Pho4-dependent manner. group 3 contains genes annotated to be other stress related. perhaps 5/15 tested are likely real targets (induced under stress). group 4 contains genes annotated as cell-wall and adhesion related, of them, just 3 (out of 19) are clearly induced.
- the analyses I did today only use the normalized log2 count per million data, without voom() transformation and no statistical tests are employed.

### Plan

- employ the LIMMA framework to formally test for differential gene expression. first do it on the pho80Δ background to compare to my previous data, then onto the PHO80 background and use the 30' time point as the post-stress, compare it to the pre-stress sample, to identify both Pho4-dependent and Pho4-independent targets.

## [25 oct 2017] Normalization and transforming data

### Normalization

_Background_

- the goal, to put it in a simple term, is to make samples comparable. Without normalization, the absolute read count for any gene is not comparable between samples, which are usually derived from different amount of starting materials, and sequenced to different depth, both factors are difficult to control. Many methods of normalization have been proposed. One group of methods share the property that they apply a _single_ scaling factor to each sample, differing among them only in how the scaling factor is estimated. They vary from the simplest library size adjustment, i.e. normalize only by the total number of reads per sample, to more sophisticated ones such as TMM (as implemented in edgeR), which uses the trimmed mean of ratios for individual genes. Another class of methods applies distribution wide adjustment. Quantile normalization is one of them. It forces the distribution of all samples to be the same by projecting each distribution to a single axis, estimated by averaging over all the distributions.

_Test_

- Follow the code snippets in [RUVSeq manual](http://bioconductor.org/packages/release/bioc/vignettes/RUVSeq/inst/doc/RUVSeq.pdf). Upperquartile normalization seems to effectively bring all samples to the same mean in the RLE plot (see manual above for explanation).
- Tested the TMM method as implemented in edgeR package `calcNormFactors()` function. But it seems it didn't do anything to the data? To be further investigated.
- According to [this paper](https://www.frontiersin.org/articles/10.3389/fgene.2016.00164/full), the "normalization factor" as given by the `calcNormFactors()` is not to be directly applied to the raw counts.

_Notes_

1. According to [Maza 2016](https://doi.org/10.3389/fgene.2016.00164), the normalization factor (call it edgeR.F) calculated by edgeR package doesn't mean the same thing as those calculated by other methods. In particular, edgeR.Fs have been adjusted so that they multiply to 1 (divided by the geometric mean of all edgeR.F's).

1. the diagnostic plot I used from the RUVSeq manual is `plotRLE`, which is not the same as a boxplot for the log transformed count data. [Gandolfo and Speed 2017](https://arxiv.org/abs/1704.03590v1) shows the procedures used to construct the RLE plot:

    > 1.  For each gene j,  calculate its median expression across the m samples,  i.e. $Med(y_{∗j})$,  then calculate the deviations from this median, i.e. calculate $y_{ij} − Med( y_{∗j} )$, across the i's.
	> 2.  For each sample, generate a boxplot of all the deviations for that sample.

1. To understand how edgeR approach the normalization problem, think of it as a two-step process. First it normalizes by the library size alone. Then, to account for biases due to RNA composition differences between samples, e.g. some highly expressed genes in particular samples "artificially suppress" the level of expression of other genes during the first normalization step, it estimates an adjustment factor based on gene-level expression log ratio values (the M values). A trimmed and weighted mean approach was used to produce a robust estimate of this adjustment factor. One can think of it as adjusting the library size to be "effective library size".

1. I found a small mistake in my code and resolved the controversy: basically TMM performs equally well as upperquartile normalization method, both of which produce slightly less homogeneous distribution across samples compared to the full quantile normalization.

## [24 oct 2017] Plan R analysis

### Goal

- Identify phosphate starvation responsive genes, both Pho4-dependent and Pho4-independent  
	-- what criteria will I use to call these genes?
- Reveal the temporal dynamics of gene induction

### Approach

- Transform the read counts by adding a small pseudo-count to avoid problems with ratio estimate.

- Test several normalization methods (TMM, quantile, upper quantile, and the new method advocated by Terry Speed)
	
- Apply voom transformation to remove dependency between expression level and variance

- Follow recipes in the `Limma` package manual on how to analyze time-course data

	Some ideas:

    - Estimate the fold-induction for each gene at every time point.
	- Manually check the gene-induction kinetics for a few positive and negative control genes. Get a sense of how fast different responses are mounted
    - Based on the above manual-evaluation, pick a time-point that satisfies the following
	
	    1. Most of the known Pho4 targets have been induced

		1. Few of the secondary response genes (how to define?) are turned on

    - To define the PHO responsive genes in _C. glabrata_
	
	    a. combine all genes that show significant induction at one or more post-stress time points (the big set), or

		a. select genes that are turned on only at the time-point selected above (first responders)

		and

		a. in both sets, use the _pho4_ deletion strain to distinguish Pho4-dependent vs Pho4-independent targets.

### Agenda

1. QC
    - check if read coverage is largely uniform across gene bodies
2. Set up for the analysis
    - load data
	- make corresponding annotation table for samples
	- some rough visualizations
3. Test adding pseudo-count to the count matrix in order to increase the accuracy in the estimates of fold change
4. Normalize 


## [18 oct 2017] Gene counting with bedtools, summary (finish work on the server's side)

- bedtools is installed on the head node, version v2.23.0

	- I would like to install v2.25.0 or newer to take advantage of the `-sorted` option, which both speeds up the process and also outputs results in the order given in the annotation file
	- After consulting with csgenomes, I was able to use 2.26.0 by calling `/usr/local/BEDTools/2.26.0/bin/bedtools`
	- I need to make a few modification on the command-line, but nothing major.
	- Resulting output files were then processed by the `Post_bedtools_analysis.R` script, which I also modified to fit the current need.
	- Manually checked _PHO84_ and _PMU2_ induction levels. Results largely are as expected, although there appears to be a very low level of induction in _$pho4 \Delta$_ strains, which is not expected.
	- Sync the results to my local folder for further analysis. 

- 
## [17 oct 2017] IGV browser, look at raw data

- managed to get x11 forwarding working

	- initially I got the "error locking $HOME/.Xauthority" error. after googling, I understand that this file must have 600 permission (read/write only by user). But mine is correct. Then I found there is another `.Xauthority-c` file. After moving it to `.Xauthority-c.bk` and reconnecting with `ssh -X gen-comp1`, x11 forwarding is now working. Tested with `xterm`
	- downloaded the latest igv browser from [broad institute](http://software.broadinstitute.org/software/igv/LoadGenome) and unzipped the folder under `$HOME/src`
	- created hard links for `igv.sh` and `igv.jar` in the `$HOME/bin` directory, and successfully launched the app
	- launched IGV and loaded the genome file I made before under `$HOME/data/NGS/Genome_Annotation/C_glabrata`
	- the GUI is very sluggish, barely usable

## [13 oct 2017] Bowtie mapping

_Goal_

- Perform bowtie mapping for all 40 samples against the latest Cgla genome

_Notes_

1. bowtie 0.12.7 is installed on gen-comp1. bowtie2 2.2.0 is also installed. According to the software's developers ["bowtie2 is not a drop-in replacement for bowtie 1"](http://bowtie-bio.sourceforge.net/bowtie2/faq.shtml). I decided to use bowtie 0.12.7 first, as all the parameters I used before work.

1. `samtools sort` often fails in my previous experience. after looking at the [documentation](http://www.htslib.org/doc/samtools-1.3.html), I figured out the problem: use of `-T /tmp/sort_tmp` flag together with submitting array jobs (multiple jobs at the same time) can create conflict between the different jobs -- one job may have just deleted the tmp file it created when another wants to access it. Solution is rather simple: just leave out the flag and let samtools use the default, which is to create temporary files along side the `sam` file. Also, use of `-m 10G` will allocate 10G of memory towards each task such that it can hold enough data in the memory and can thus skip the merging step.

_Results_

- average uniquely mapped reads percentage is ~90%. The lowest (S10) is 82%

## [12 oct 2017] Demultiplex

- To demultiplex, one needs to go to the individual assay page, find the button that says "create custom barcodes", and follow the instructions.

- Pool A is sequenced together with 5 other samples. The i7 adapter, which contains the TruSeq index, conflicts between my sample and those 5 samples. Wang Wei at the core pointed out that my sample and the other person's differ in the i5 index, which is read as Read 3, and can be used to distinguish our samples from each other. My i5 index is TCTTTCCC, while the other party's is GGTTGAGA.

- The HTSEQ server most likely uses [bcl2fastq2](https://support.illumina.com/content/dam/illumina-support/documents/downloads/software/bcl2fastq/bcl2fastq2-v2-18-software-guide-15051736-01.pdf) from illumina to do the demultiplexing. This software has a flag `--barcode-mismatches` that can be set to 0, 1 or 2, with the default being 1. Wei suggested using 2 for both i5 and i7 adapters.

- Some thoughts on the mismatches allowed:

	Calculate hamming distance between TruSeq adapter indices 1-20

	```{r test}
	require(stringdist)
	seq <- c("ATCACG", "CGATGT", "TTAGGC", "TGACCA", "ACAGTG", "GCCAAT", "CAGATC", "ACTTGA", "GATCAG", "TAGCTT", "GGCTAC", "CTTGTA", "AGTCAA", "AGTTCC", "ATGTCA", "CCGTCC", "GTCCGC", "GTGAAA", "GTGGCC", "GTTTCG")
	dist <- stringdistmatrix(seq, seq, method = "hamming")
	print("The distribution of pairwise hamming distance is:")
	print(table(dist))
	print("Note that the 20 zeros are self-distance")
	```
	
	output

	```
	dist
	0   3   4   5   6 
	20  56  98 166  60
	```
	
	It is important to realize that allowing more mismatches won't dramatically inflate the rate of index misassignment, i.e. read index 1 as index 2. This is because any index read that is equal distance away from the supplied set of indices used will be discarded.

- Results of demultiplex

    - PoolA is co-sequenced with 5 other samples that share the i7 barcodes with 3 of my samples. As a result, I need to utilize the i5 adapter sequence, which was in Read 3. This dual index demultiplexing is more complex and resulted in a higher percentage of unmatched data. When allowing 1 mismatch (on each of i5 and i7 index), ~35% of the reads are dropped into the "unmatched" bin. When allowing 2 mismatches, ~25% fall in the "unmatched" bin.

    - On PoolB, which is only multiplexed with one other sample, I used a more stringent cutoff of 1 mismatch, and the percentage of unmatched reads (due to indices mismatch) is &lt 1%. Totally fine.

## [11 oct 2017] Sequencing completed

- Data available on HTSEQ.princeton.edu

- The HTSEQ website is essentially a front end for a MySQL database. It organizes all the sample information and run results using concepts including _Sample_, _Assay_, _Dataset_ etc. A sample is what users submit to the core. An assay refers to a run on the Illumina machine. Note that for the Rapid Flowcell run on HiSeq 2500, each run has two assays, although it would be the same pooled samples sequenced in both. They result in a "merged" fastq.
